{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CLIP model loaded successfully\n",
      "Processing image: red_hoodie.jpeg\n",
      "Text prompt: A bright red hoodie\n",
      "Similarity between image and text: 28.10%\n",
      "Search query: A bright red hoodie\n",
      "Full search response: {'Abstract': '', 'AbstractSource': '', 'AbstractText': '', 'AbstractURL': '', 'Answer': '', 'AnswerType': '', 'Definition': '', 'DefinitionSource': '', 'DefinitionURL': '', 'Entity': '', 'Heading': '', 'Image': '', 'ImageHeight': '', 'ImageIsLogo': '', 'ImageWidth': '', 'Infobox': '', 'Redirect': '', 'RelatedTopics': [], 'Results': [], 'Type': '', 'meta': {'attribution': None, 'blockgroup': None, 'created_date': '2021-03-24', 'description': 'testing', 'designer': None, 'dev_date': '2021-03-24', 'dev_milestone': 'development', 'developer': [{'name': 'zt', 'type': 'duck.co', 'url': 'https://duck.co/user/zt'}], 'example_query': '', 'id': 'just_another_test', 'is_stackexchange': 0, 'js_callback_name': 'another_test', 'live_date': None, 'maintainer': {'github': ''}, 'name': 'Just Another Test', 'perl_module': 'DDG::Lontail::AnotherTest', 'producer': None, 'production_state': 'offline', 'repo': 'fathead', 'signal_from': 'just_another_test', 'src_domain': 'how about there', 'src_id': None, 'src_name': 'hi there', 'src_options': {'directory': '', 'is_fanon': 0, 'is_mediawiki': 0, 'is_wikipedia': 0, 'language': '', 'min_abstract_length': None, 'skip_abstract': 0, 'skip_abstract_paren': 0, 'skip_icon': 0, 'skip_image_name': 0, 'skip_qr': '', 'src_info': '', 'src_skip': ''}, 'src_url': 'Hello there', 'status': None, 'tab': 'is this source', 'topic': [], 'unsafe': None}}\n",
      "[]\n",
      "Found 0 search results\n",
      "\n",
      "Top filtered results:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "import numpy as np\n",
    "from torchvision.transforms import RandomHorizontalFlip, ColorJitter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load CLIP model\n",
    "model, clip_preprocess = clip.load(\"ViT-L/14\", device=device, jit=False)\n",
    "print(\"CLIP model loaded successfully\")\n",
    "\n",
    "# Custom preprocessing function\n",
    "def preprocess(image):\n",
    "    transform = Compose([\n",
    "        Resize(224),\n",
    "        CenterCrop(224),\n",
    "        RandomHorizontalFlip(p=0.0),\n",
    "        ColorJitter(brightness=0.1, contrast=0.2),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0).to(device)\n",
    "\"\"\"def preprocess(image):\n",
    "    return clip_preprocess(image).unsqueeze(0).to(device)\"\"\"\n",
    "\n",
    "# Function to encode image\n",
    "def encode_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_input = preprocess(image)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "    #print(image_features)\n",
    "    return image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Function to encode text\n",
    "def encode_text(text):\n",
    "    text_input = clip.tokenize([text]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_input)\n",
    "    return text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\"\"\"def encode_multiple_texts(prompts):\n",
    "    text_inputs = clip.tokenize(prompts).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    return text_features.mean(dim=0, keepdim=True)\"\"\"\n",
    "\n",
    "# Function to compute similarity\n",
    "\"\"\"def compute_similarity(image_features, text_features):\n",
    "    return (100.0 * image_features @ text_features.T).item()\"\"\"\n",
    "def compute_similarity(image_features, text_features):\n",
    "    return F.cosine_similarity(image_features, text_features).item() * 100\n",
    "\n",
    "# Function to perform web search\n",
    "def perform_web_search(query):\n",
    "    url = f\"https://api.duckduckgo.com/?q={query}&format=json&pretty=1\"\n",
    "    response = requests.get(url)\n",
    "    results = response.json()\n",
    "    \n",
    "    print(\"Full search response:\", results)  # Added for debugging\n",
    "    \n",
    "    return results.get('RelatedTopics', [])\n",
    "\n",
    "\n",
    "# Function to filter results\n",
    "def filter_results(search_results, original_image_features, threshold=0.1):\n",
    "    filtered_results = []\n",
    "    for result in search_results:\n",
    "        # Check if there's a valid image URL in the result\n",
    "        if 'Icon' in result and 'URL' in result['Icon']:\n",
    "            result_image_url = result['Icon']['URL']\n",
    "            \n",
    "            # Ensure the URL is not empty and starts with 'http' or 'https'\n",
    "            if result_image_url and result_image_url.startswith(('http', 'https')):\n",
    "                try:\n",
    "                    response = requests.get(result_image_url)\n",
    "                    result_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "                    result_image_input = preprocess(result_image)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        result_features = model.encode_image(result_image_input)\n",
    "                        result_features /= result_features.norm(dim=-1, keepdim=True)\n",
    "                    \n",
    "                    similarity = compute_similarity(original_image_features, result_features)\n",
    "                    \n",
    "                    if similarity > threshold:\n",
    "                        filtered_results.append({\n",
    "                            'url': result['FirstURL'],\n",
    "                            'title': result['Text'],\n",
    "                            'similarity': similarity\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing result image from URL '{result_image_url}': {e}\")\n",
    "        else:\n",
    "            print(f\"Skipping result due to missing or invalid image URL.\")\n",
    "    \n",
    "    return sorted(filtered_results, key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main(image_path, text_prompt):\n",
    "    print(f\"Processing image: {image_path}\")\n",
    "    print(f\"Text prompt: {text_prompt}\")\n",
    "    \n",
    "    # Encode image and text\n",
    "    image_features = encode_image(image_path)\n",
    "    text_features = encode_text(text_prompt)\n",
    "    #text_features = encode_multiple_texts(text_prompt)\n",
    "    \n",
    "    # Compute similarity between image and text\n",
    "    similarity = compute_similarity(image_features, text_features)\n",
    "    print(f\"Similarity between image and text: {similarity:.2f}%\")\n",
    "    \n",
    "    # Generate search query\n",
    "    search_query = text_prompt  # For simplicity, we're using the text prompt as the search query\n",
    "    print(f\"Search query: {search_query}\")\n",
    "    \n",
    "    # Perform web search\n",
    "    search_results = perform_web_search(search_query)\n",
    "    print(search_results)\n",
    "    print(f\"Found {len(search_results)} search results\")\n",
    "    \n",
    "    # Filter results\n",
    "    filtered_results = filter_results(search_results, image_features)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nTop filtered results:\")\n",
    "    for i, result in enumerate(filtered_results[:5], 1):\n",
    "        print(f\"{i}. Title: {result['title']}\")\n",
    "        print(f\"   URL: {result['url']}\")\n",
    "        print(f\"   Similarity: {result['similarity']:.2f}%\")\n",
    "        print(\"---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    image_path = \"red_hoodie.jpeg\"  # Replace with your image path\n",
    "    #text_prompt = [\"red hoodie\",\"a bright red hoodie\"]\n",
    "    text_prompt=\"A bright red hoodie\"\n",
    "\n",
    "    main(image_path, text_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CLIP model loaded successfully\n",
      "Processing image: red_hoodie.jpeg\n",
      "Text prompt: red hoodie\n",
      "Similarity between image and text: 32.16%\n",
      "Search query: red hoodie\n",
      "Found 0 search results\n",
      "\n",
      "Top filtered results:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load CLIP model\n",
    "model, clip_preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "print(\"CLIP model loaded successfully\")\n",
    "\n",
    "# Custom preprocessing function\n",
    "def preprocess(image):\n",
    "    return clip_preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "# Function to encode image\n",
    "def encode_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_input = preprocess(image)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "    return image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Function to encode text\n",
    "def encode_text(text):\n",
    "    text_input = clip.tokenize([text]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_input)\n",
    "    return text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Function to compute similarity\n",
    "def compute_similarity(image_features, text_features):\n",
    "    return (100.0 * image_features @ text_features.T).item()\n",
    "\n",
    "# Function to perform web search (DuckDuckGo API)\n",
    "def perform_web_search(query):\n",
    "    url = f\"https://api.duckduckgo.com/?q={query}&format=json&pretty=1\"\n",
    "    response = requests.get(url)\n",
    "    results = response.json()\n",
    "    return results.get('RelatedTopics', [])\n",
    "\n",
    "# Function to filter results by comparing both image and text features\n",
    "def filter_results(search_results, original_image_features, threshold=0.1):\n",
    "    filtered_results = []\n",
    "    for result in search_results:\n",
    "        # Check if there's a valid image URL in the result\n",
    "        if 'Icon' in result and 'URL' in result['Icon']:\n",
    "            result_image_url = result['Icon']['URL']\n",
    "            \n",
    "            # Ensure the URL is not empty and starts with 'http' or 'https'\n",
    "            if result_image_url and result_image_url.startswith(('http', 'https')):\n",
    "                try:\n",
    "                    # Download and preprocess the image from the search result\n",
    "                    response = requests.get(result_image_url)\n",
    "                    result_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "                    result_image_input = preprocess(result_image)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        # Encode the image from the search result\n",
    "                        result_features = model.encode_image(result_image_input)\n",
    "                        result_features /= result_features.norm(dim=-1, keepdim=True)\n",
    "                    \n",
    "                    # Compute image similarity\n",
    "                    image_similarity = compute_similarity(original_image_features, result_features)\n",
    "                    \n",
    "                    # Filter based on image similarity\n",
    "                    if image_similarity > threshold:\n",
    "                        filtered_results.append({\n",
    "                            'url': result['FirstURL'],\n",
    "                            'title': result['Text'],\n",
    "                            'image_similarity': image_similarity\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing result image from URL '{result_image_url}': {e}\")\n",
    "        else:\n",
    "            print(f\"Skipping result due to missing or invalid image URL.\")\n",
    "    \n",
    "    return sorted(filtered_results, key=lambda x: x['image_similarity'], reverse=True)\n",
    "\n",
    "# Main function to process both image and text for searching\n",
    "def main(image_path, text_prompt):\n",
    "    print(f\"Processing image: {image_path}\")\n",
    "    print(f\"Text prompt: {text_prompt}\")\n",
    "    \n",
    "    # Encode both image and text\n",
    "    image_features = encode_image(image_path)\n",
    "    text_features = encode_text(text_prompt)\n",
    "    \n",
    "    # Compute similarity between image and text (for reference)\n",
    "    similarity = compute_similarity(image_features, text_features)\n",
    "    print(f\"Similarity between image and text: {similarity:.2f}%\")\n",
    "    \n",
    "    # Optionally: generate caption from image (or describe image manually)\n",
    "    # For now, we're not using image captioning, but you could use models like BLIP or Donut\n",
    "    \n",
    "    # Combine the user text with some description about the image\n",
    "    # This could be a description from the image captioning model\n",
    "    search_query = f\"{text_prompt}\"  # You can expand this with extracted image captions\n",
    "    print(f\"Search query: {search_query}\")\n",
    "    \n",
    "    # Perform web search based on the text prompt\n",
    "    search_results = perform_web_search(search_query)\n",
    "    print(f\"Found {len(search_results)} search results\")\n",
    "    \n",
    "    # Filter results based on image similarity\n",
    "    filtered_results = filter_results(search_results, image_features)\n",
    "    \n",
    "    # Display the top filtered results\n",
    "    print(\"\\nTop filtered results:\")\n",
    "    for i, result in enumerate(filtered_results[:5], 1):\n",
    "        print(f\"{i}. Title: {result['title']}\")\n",
    "        print(f\"   URL: {result['url']}\")\n",
    "        print(f\"   Image Similarity: {result['image_similarity']:.2f}%\")\n",
    "        print(\"---\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"red_hoodie.jpeg\"  # Replace with the path to your image\n",
    "    text_prompt = \"red hoodie\"      # Replace with your text query\n",
    "    main(image_path, text_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CLIP model loaded successfully\n",
      "Processing image: red_hoodie.jpeg\n",
      "Text prompt: red hoodie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pushp_vnmc973\\Documents\\ML\\PS1\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between image and text: 32.16%\n",
      "Search query: red hoodie\n",
      "Found 0 search results\n",
      "\n",
      "Top filtered results:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load CLIP model\n",
    "model, clip_preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "print(\"CLIP model loaded successfully\")\n",
    "\n",
    "# Custom preprocessing function\n",
    "def preprocess(image):\n",
    "    return clip_preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "# Function to encode image\n",
    "def encode_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_input = preprocess(image)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "    return image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Function to encode text\n",
    "def encode_text(text):\n",
    "    text_input = clip.tokenize([text]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_input)\n",
    "    return text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Function to compute similarity\n",
    "def compute_similarity(image_features, text_features):\n",
    "    return (100.0 * image_features @ text_features.T).item()\n",
    "\n",
    "# Function to perform web search (DuckDuckGo API)\n",
    "def perform_web_search(query):\n",
    "    url = f\"https://api.duckduckgo.com/?q={query}&format=json&pretty=1\"\n",
    "    response = requests.get(url)\n",
    "    results = response.json()\n",
    "    return results.get('RelatedTopics', [])\n",
    "\n",
    "# Function to filter results by comparing both image and text features\n",
    "def filter_results(search_results, original_image_features, original_text_features, threshold=0.0):\n",
    "    filtered_results = []\n",
    "    for result in search_results:\n",
    "        # Check if there's a valid image URL in the result\n",
    "        if 'Icon' in result and 'URL' in result['Icon']:\n",
    "            result_image_url = result['Icon']['URL']\n",
    "            \n",
    "            # Ensure the URL is not empty and starts with 'http' or 'https'\n",
    "            if result_image_url and result_image_url.startswith(('http', 'https')):\n",
    "                try:\n",
    "                    # Download and preprocess the image from the search result\n",
    "                    response = requests.get(result_image_url)\n",
    "                    result_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "                    result_image_input = preprocess(result_image)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        # Encode the image from the search result\n",
    "                        result_features = model.encode_image(result_image_input)\n",
    "                        result_features /= result_features.norm(dim=-1, keepdim=True)\n",
    "                    \n",
    "                    # Compute image similarity\n",
    "                    image_similarity = compute_similarity(original_image_features, result_features)\n",
    "                    \n",
    "                    # Combine image similarity with text similarity for better ranking\n",
    "                    text_similarity = compute_similarity(original_text_features, original_text_features)\n",
    "                    combined_similarity = (image_similarity + text_similarity) / 2\n",
    "                    \n",
    "                    # Filter based on combined similarity\n",
    "                    if combined_similarity > threshold:\n",
    "                        filtered_results.append({\n",
    "                            'url': result['FirstURL'],\n",
    "                            'title': result['Text'],\n",
    "                            'image_similarity': image_similarity,\n",
    "                            'text_similarity': text_similarity,\n",
    "                            'combined_similarity': combined_similarity\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing result image from URL '{result_image_url}': {e}\")\n",
    "        else:\n",
    "            print(f\"Skipping result due to missing or invalid image URL.\")\n",
    "    \n",
    "    return sorted(filtered_results, key=lambda x: x['combined_similarity'], reverse=True)\n",
    "\n",
    "# Main function to process both image and text for searching\n",
    "def main(image_path, text_prompt):\n",
    "    print(f\"Processing image: {image_path}\")\n",
    "    print(f\"Text prompt: {text_prompt}\")\n",
    "    \n",
    "    # Encode both image and text\n",
    "    image_features = encode_image(image_path)\n",
    "    text_features = encode_text(text_prompt)\n",
    "    \n",
    "    # Compute similarity between image and text (for reference)\n",
    "    similarity = compute_similarity(image_features, text_features)\n",
    "    print(f\"Similarity between image and text: {similarity:.2f}%\")\n",
    "    \n",
    "    # Generate search query using the text prompt\n",
    "    search_query = text_prompt  # Using the text prompt for search query\n",
    "    print(f\"Search query: {search_query}\")\n",
    "    \n",
    "    # Perform web search based on the text prompt\n",
    "    search_results = perform_web_search(search_query)\n",
    "    print(f\"Found {len(search_results)} search results\")\n",
    "    \n",
    "    # Filter results based on both image and text similarities\n",
    "    filtered_results = filter_results(search_results, image_features, text_features)\n",
    "    \n",
    "    # Display the top filtered results\n",
    "    print(\"\\nTop filtered results:\")\n",
    "    for i, result in enumerate(filtered_results[:5], 1):\n",
    "        print(f\"{i}. Title: {result['title']}\")\n",
    "        print(f\"   URL: {result['url']}\")\n",
    "        print(f\"   Image Similarity: {result['image_similarity']:.2f}%\")\n",
    "        print(f\"   Text Similarity: {result['text_similarity']:.2f}%\")\n",
    "        print(f\"   Combined Similarity: {result['combined_similarity']:.2f}%\")\n",
    "        print(\"---\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"red_hoodie.jpeg\"  # Replace with the path to your image\n",
    "    text_prompt = \"red hoodie\"      # Replace with your text query\n",
    "    main(image_path, text_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# contextual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "def image_to_text(image_path):\n",
    "    # Load the BLIP model and processor\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image_path = \"red_hoodie.jpeg\"  # Replace with your image path\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Preprocess the image and generate caption\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs)\n",
    "\n",
    "    # Decode the generated caption\n",
    "    caption = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return f\"Generated Caption: {caption}\"\n",
    "\n",
    "\"\"\"def perform_web_search(query):\n",
    "    url = f\"https://api.duckduckgo.com/?q={query}&format=json&pretty=1\"\n",
    "    response = requests.get(url)\n",
    "    results = response.json()\n",
    "    return results.get('RelatedTopics', [])\"\"\"\n",
    "\n",
    "import requests\n",
    "\n",
    "# Define the function for web search using ContextualWebSearch API\n",
    "def perform_web_search(query, num_results=5):\n",
    "    url = \"https://contextualwebsearch-websearch-v1.p.rapidapi.com/api/Search/WebSearchAPI\"\n",
    "    \n",
    "    # Parameters for the search\n",
    "    params = {\n",
    "        \"q\": query,              # The search query\n",
    "        \"pageNumber\": \"1\",       # Page number of the results\n",
    "        \"pageSize\": num_results, # Number of results to return\n",
    "        \"autoCorrect\": \"true\"    # Enable autocorrect in the search\n",
    "    }\n",
    "    \n",
    "    # Headers with API key\n",
    "    headers = {\n",
    "        \"X-RapidAPI-Key\": \"YOUR_API_KEY\",  # Replace with your actual API key\n",
    "        \"X-RapidAPI-Host\": \"contextualwebsearch-websearch-v1.p.rapidapi.com\"\n",
    "    }\n",
    "    \n",
    "    # Send the request\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    \n",
    "    # If the request was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        results = data.get('value', [])\n",
    "        \n",
    "        # Extracting the title, description, and URL from the search results\n",
    "        for idx, result in enumerate(results, 1):\n",
    "            title = result.get('title')\n",
    "            description = result.get('description')\n",
    "            url = result.get('url')\n",
    "            print(f\"{idx}. {title}\\nDescription: {description}\\nURL: {url}\\n\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    image_path = \"red_hoodie.jpeg\"  # Replace with your image path\n",
    "    #text_prompt = [\"red hoodie\",\"a bright red hoodie\"]\n",
    "    text_prompt=\"search for this types\"\n",
    "    search_query=image_to_text(image_path) + text_prompt\n",
    "\n",
    "    perform_web_search(search_query,num_results=5)\n",
    "    #print(f\"Found {len(search_results)} search results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mojeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 404, {\"message\":\"API doesn't exists\"}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Function to generate a text caption from an image using BLIP\n",
    "def image_to_text(image_path):\n",
    "    # Load the BLIP model and processor\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Preprocess the image and generate caption\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs)\n",
    "\n",
    "    # Decode the generated caption\n",
    "    caption = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# Function to perform web search using Mojeek API\n",
    "def perform_web_search(query):\n",
    "    url = \"https://www.mojeek.com/search\"\n",
    "    \n",
    "    params = {\n",
    "        \"q\": query,              # The search query\n",
    "        \"count\": \"5\",            # Number of results to return\n",
    "        \"output\": \"json\"         # Specify that the response should be in JSON format\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer YOUR_API_KEY\"  # Replace with your actual Mojeek API key\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    \n",
    "    # Check for errors\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        # Print out the search results\n",
    "        for idx, result in enumerate(results, 1):\n",
    "            title = result.get('title')\n",
    "            description = result.get('snippet')\n",
    "            link = result.get('url')\n",
    "            print(f\"{idx}. {title}\\nDescription: {description}\\nURL: {link}\\n\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    image_path = \"red_hoodie.jpeg\"  # Replace with your image path\n",
    "    caption = image_to_text(image_path)\n",
    "    text_prompt = \"search for this type of product\"\n",
    "    \n",
    "    # Combine the image caption with the text prompt to form a search query\n",
    "    search_query = caption + \" \" + text_prompt\n",
    "\n",
    "    # Perform web search\n",
    "    perform_web_search(search_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# duckduckgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pushp_vnmc973\\Documents\\ML\\PS1\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a red hoodie sweatshirt with a white hoodie\n",
      "Found 0 search results\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "def image_to_text(image_path):\n",
    "    # Load the BLIP model and processor\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image_path = \"red_hoodie.jpeg\"  # Replace with your image path\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Preprocess the image and generate caption\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs)\n",
    "\n",
    "    # Decode the generated caption\n",
    "    caption = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(caption)\n",
    "    return caption\n",
    "\n",
    "# Define the function for web search using ContextualWebSearch API\n",
    "def perform_web_search(query):\n",
    "    url = f\"https://api.duckduckgo.com/?q={query}&format=json&pretty=1\"\n",
    "    response = requests.get(url)\n",
    "    results = response.json()\n",
    "    return results.get('RelatedTopics', [])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    image_path = \"red_hoodie.jpeg\"  # Replace with your image path\n",
    "    #text_prompt = [\"red hoodie\",\"a bright red hoodie\"]\n",
    "    text_prompt=\"search for this types\"\n",
    "    search_query=image_to_text(image_path) + text_prompt\n",
    "\n",
    "    search_results=perform_web_search(search_query)\n",
    "    print(f\"Found {len(search_results)} search results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
